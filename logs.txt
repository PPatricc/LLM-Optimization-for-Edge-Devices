$ python baseline.py
2025-03-08 15:28:32,612 [INFO] Using device: cpu
2025-03-08 15:28:32,612 [INFO] Loading SST-2 dataset...
2025-03-08 15:28:39,492 [INFO] Reducing training set to 5000 samples.
Map: 100%|##########| 872/872 [00:00<00:00, 10185.31 examples/s]
2025-03-08 15:28:39,773 [INFO] Loading base model: distilbert-base-uncased
2025-03-08 15:28:40,597 [INFO] Training the base model for 3 epochs on data...
2025-03-08 15:52:47,948 [INFO] Epoch 1/3, Loss: 0.3545
2025-03-08 16:15:50,298 [INFO] Epoch 2/3, Loss: 0.1474
2025-03-08 16:38:51,106 [INFO] Epoch 3/3, Loss: 0.0727
2025-03-08 16:38:51,106 [INFO] Measuring baseline performance...
2025-03-08 16:40:04,340 [INFO] Baseline metrics: {'model_size_mb': 255.41309356689453, 'avg_latency_seconds': 0.6703090317752383, 'accuracy': 0.8899082568807339}

$ python knowledge_distillation.py
2025-03-08 18:09:21,092 [INFO] Using device: cpu
2025-03-08 18:09:21,092 [INFO] Loading SST-2 dataset...
2025-03-08 18:09:28,066 [INFO] Reducing training set to 5000 samples.
2025-03-08 18:17:49,368 [INFO] Epoch 1/5, Distillation Loss: 1.4008
2025-03-08 18:26:03,665 [INFO] Epoch 2/5, Distillation Loss: 1.0093
2025-03-08 18:34:28,526 [INFO] Epoch 3/5, Distillation Loss: 0.7575
2025-03-08 18:45:31,251 [INFO] Epoch 4/5, Distillation Loss: 0.6105
2025-03-08 18:54:57,838 [INFO] Epoch 5/5, Distillation Loss: 0.5092
2025-03-08 18:55:00,619 [INFO] Distilled student model size: 16.73 MB, accuracy: 0.768

$ python pruning_quantization.py
2025-03-08 18:56:57,806 [INFO] Using device: cpu
2025-03-08 18:56:57,806 [INFO] Loading SST-2 dataset...
2025-03-08 18:57:07,926 [INFO] Reducing training set to 5000 samples.
2025-03-08 18:57:08,632 [INFO] Loading baseline model from data/baseline_outputs/baseline_model
2025-03-08 18:57:12,719 [INFO] Applying GRADUAL pruning...
2025-03-08 19:18:34,789 [INFO] Gradual pruning step 1/3, total prune ratio ~0.10
2025-03-08 19:39:29,851 [INFO] Gradual pruning step 2/3, total prune ratio ~0.20
2025-03-08 20:28:05,079 [INFO] Gradual pruning step 3/3, total prune ratio ~0.30
2025-03-08 20:31:55,770 [INFO] [Gradual-Pruned] Disk size: 256.35 MB, accuracy: 0.845
2025-03-08 20:31:55,777 [INFO] Applying dynamic quantization on linear layers.
2025-03-08 20:33:44,864 [INFO] [Gradual-Pruned+Quant] Disk size: 132.29 MB, accuracy: 0.833

$ python evaluate_compare.py
2025-03-09 11:49:09,386 [INFO] Using device: cpu
2025-03-09 11:49:09,387 [INFO] Loading SST-2 dataset...
2025-03-09 11:49:16,800 [INFO] Reducing training set to 5000 samples.
2025-03-09 11:49:17,054 [INFO] Baseline (trained) => size: 255.41 MB, accuracy: 0.890
2025-03-09 11:49:17,061 [INFO] Distilled => size: 16.73 MB, accuracy: 0.768
2025-03-09 11:49:17,070 [INFO] Pruned => size on disk: 256.35 MB, acc: 0.845
2025-03-09 11:49:17,070 [INFO] Pruned+Quant => size on disk: 132.29 MB, acc: 0.833
2025-03-09 11:49:17,075 [INFO] [Distilled] Size reduction: 93.45%, Acc retention: 86.34%
2025-03-09 11:49:17,075 [INFO] [Pruned] Size reduction: -0.37%, Acc retention: 94.97%
2025-03-09 11:49:17,075 [INFO] [Pruned+Quant] Size reduction: 48.21%, Acc retention: 93.56%
